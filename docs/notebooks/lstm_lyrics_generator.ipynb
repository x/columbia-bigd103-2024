{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"L4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# Using an LSTM to Generate Songs\n","\n","In this exercise, we're going to build a _small_ language model. Something that learns a sequence of _characters_ and can use it to generate text.\n","\n","Think of it as a fancy version of our Markov chains.\n","\n","\n","## Imports and Set Up\n","\n","```python\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import Dataset, DataLoader\n","import pandas as pd\n","import numpy as np\n","from google.colab import drive\n","\n","# Mount Google Drive\n","drive.mount('/content/gdrive')\n","\n","# Set the device\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(f\"Using device: {device}\")\n","```"],"metadata":{"id":"Q1GzdTqifoaF"}},{"cell_type":"code","execution_count":1,"metadata":{"id":"FpKumcg9bqer","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1720729213301,"user_tz":240,"elapsed":4173,"user":{"displayName":"Devon Peticolas","userId":"04023788229320726045"}},"outputId":"c06c6e0b-da42-4640-ed2c-1150eac37d7e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Using device: cuda\n"]}],"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import Dataset, DataLoader\n","import pandas as pd\n","import numpy as np\n","from google.colab import drive\n","\n","# # Mount Google Drive\n","# drive.mount('/content/gdrive')\n","\n","# Set the device\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(f\"Using device: {device}\")\n"]},{"cell_type":"markdown","source":["# Load our Dataset\n","\n","Pick an artist you want to train on...\n","\n","```python\n","print(df[\"Artist\"].unique())\n","```\n","\n","And then filter to that artist (I chose \"Taylor Swift\" as an example).\n","\n","```python\n","# Load the dataset\n","df = pd.read_csv('/content/gdrive/MyDrive/datasets/songs.csv')\n","df = df[df[\"Artist\"] == \"Taylor Swift\"]\n","lyrics = df['Lyrics'].str.cat(sep='\\n')\n","\n","# Display the first few lines of the lyrics\n","print(lyrics[:500])\n","```"],"metadata":{"id":"2XtXlCLRggqC"}},{"cell_type":"code","source":["# Load the dataset\n","df = pd.read_csv('songs.csv')\n","df = df[df[\"Artist\"] == \"Taylor Swift\"]\n","lyrics = df['Lyrics'].str.cat(sep='\\n')\n","\n","# Display the first few lines of the lyrics\n","print(lyrics[:500])"],"metadata":{"id":"MdAVYHSIgkU7","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1720729234180,"user_tz":240,"elapsed":224,"user":{"displayName":"Devon Peticolas","userId":"04023788229320726045"}},"outputId":"1d09f0a5-4030-41c6-89b3-bf2b8522467b"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Vintage tee, brand new phone\n","High heels on cobblestones\n","When you are young, they assume you know nothing\n","Sequin smile, black lipstick\n","Sensual politics\n","When you are young, they assume you know nothing\n","\n","But I knew you\n","Dancin' in your Levi's\n","Drunk under a streetlight, I\n","I knew you\n","Hand under my sweatshirt\n","Baby, kiss it better, I\n","\n","And when I felt like I was an old cardigan\n","Under someone's bed\n","You put me on and said I was your favorite\n","\n","A friend to all is a friend to none\n","Chase two girls, lose the on\n"]}]},{"cell_type":"markdown","source":["# Preprocess and Build a Dataset\n","\n","This code is going to preprocess our lyrics and build the dataset.\n","\n","Don't feel like you need to fully understand it, but if you read through it, you'll see that we're converting all our characters into numbers. These will be how we translate our text into numeric feature vectors.\n","\n","Here's the preprocessing...\n","\n","```python\n","# Create a character-level vocabulary\n","chars = sorted(list(set(lyrics)))\n","char_to_idx = {char: idx for idx, char in enumerate(chars)}\n","idx_to_char = {idx: char for idx, char in enumerate(chars)}\n","\n","# Convert lyrics to indices\n","lyrics_idx = [char_to_idx[char] for char in lyrics]\n","\n","# Define sequence length\n","seq_length = 100\n","step = 1\n","\n","sequences = []\n","next_chars = []\n","\n","for i in range(0, len(lyrics_idx) - seq_length, step):\n","    sequences.append(lyrics_idx[i:i + seq_length])\n","    next_chars.append(lyrics_idx[i + seq_length])\n","\n","print(f'Number of sequences: {len(sequences)}')\n","\n","# Convert to numpy arrays\n","X = np.zeros((len(sequences), seq_length), dtype=int)\n","y = np.zeros((len(sequences)), dtype=int)\n","\n","for i, seq in enumerate(sequences):\n","    X[i] = seq\n","    y[i] = next_chars[i]\n","```\n","\n","And then we build the dataset class.\n","\n","```python\n","class LyricsDataset(Dataset):\n","    def __init__(self, X, y):\n","        self.X = torch.tensor(X, dtype=torch.long).to(device)\n","        self.y = torch.tensor(y, dtype=torch.long).to(device)\n","\n","    def __len__(self):\n","        return len(self.X)\n","\n","    def __getitem__(self, idx):\n","        return self.X[idx], self.y[idx]\n","\n","dataset = LyricsDataset(X, y)\n","dataloader = DataLoader(dataset, batch_size=64, shuffle=True)\n","```"],"metadata":{"id":"TUpbS4ZOgk8f"}},{"cell_type":"code","source":["# Create a character-level vocabulary|\n","chars = sorted(list(set(lyrics)))\n","char_to_idx = {char: idx for idx, char in enumerate(chars)}\n","idx_to_char = {idx: char for idx, char in enumerate(chars)}\n","\n","# Convert lyrics to indices\n","lyrics_idx = [char_to_idx[char] for char in lyrics]\n","\n","# Define sequence length\n","seq_length = 100\n","step = 1\n","\n","sequences = []\n","next_chars = []\n","\n","for i in range(0, len(lyrics_idx) - seq_length, step):\n","    sequences.append(lyrics_idx[i:i + seq_length])\n","    next_chars.append(lyrics_idx[i + seq_length])\n","\n","print(f'Number of sequences: {len(sequences)}')\n","\n","# Convert to numpy arrays\n","X = np.zeros((len(sequences), seq_length), dtype=int)\n","y = np.zeros((len(sequences)), dtype=int)\n","\n","for i, seq in enumerate(sequences):\n","    X[i] = seq\n","    y[i] = next_chars[i]\n","\n","class LyricsDataset(Dataset):\n","    def __init__(self, X, y):\n","        self.X = torch.tensor(X, dtype=torch.long).to(device)\n","        self.y = torch.tensor(y, dtype=torch.long).to(device)\n","\n","    def __len__(self):\n","        return len(self.X)\n","\n","    def __getitem__(self, idx):\n","        return self.X[idx], self.y[idx]\n","\n","dataset = LyricsDataset(X, y)\n","dataloader = DataLoader(dataset, batch_size=64, shuffle=True)\n","\n","\n"],"metadata":{"id":"G04n0L3iiPHm","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1720729245333,"user_tz":240,"elapsed":1247,"user":{"displayName":"Devon Peticolas","userId":"04023788229320726045"}},"outputId":"73d13a65-93a1-403c-9b89-ff9b8703d280"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Number of sequences: 98256\n"]}]},{"cell_type":"markdown","source":["# Build the LSTM Network\n","\n","```python\n","class LyricsLSTM(nn.Module):\n","    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers):\n","        super(LyricsLSTM, self).__init__()\n","        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n","        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers, batch_first=True)\n","        self.fc = nn.Linear(hidden_dim, vocab_size)\n","\n","    def forward(self, x, hidden):\n","        x = self.embedding(x)\n","        out, hidden = self.lstm(x, hidden)\n","        out = self.fc(out[:, -1, :])\n","        return out, hidden\n","\n","    def init_hidden(self, batch_size):\n","        weight = next(self.parameters()).data\n","        hidden = (weight.new(self.lstm.num_layers, batch_size, self.lstm.hidden_size).zero_(),\n","                  weight.new(self.lstm.num_layers, batch_size, self.lstm.hidden_size).zero_())\n","        return hidden\n","```\n","\n","And initialize our model (feel free to play with the hyper parameters here.\n","\n","```python\n","# Hyperparameters\n","vocab_size = len(chars)\n","embedding_dim = 128\n","hidden_dim = 256\n","num_layers = 2\n","\n","model = LyricsLSTM(vocab_size, embedding_dim, hidden_dim, num_layers)\n","```"],"metadata":{"id":"buZ0mQabiPzp"}},{"cell_type":"markdown","source":["# Initialize our Loss Function and Optimizer\n","\n","```python\n","optimizer = optim.Adam(model.parameters(), lr=0.001)\n","criterion = nn.CrossEntropyLoss()\n","```"],"metadata":{"id":"ot6tOWWlidXw"}},{"cell_type":"code","source":["class LyricsLSTM(nn.Module):\n","    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers):\n","        super(LyricsLSTM, self).__init__()\n","        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n","        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers, batch_first=True)\n","        self.fc = nn.Linear(hidden_dim, vocab_size)\n","\n","    def forward(self, x, hidden):\n","        x = self.embedding(x)\n","        out, hidden = self.lstm(x, hidden)\n","        out = self.fc(out[:, -1, :])\n","        return out, hidden\n","\n","    def init_hidden(self, batch_size):\n","        weight = next(self.parameters()).data\n","        hidden = (weight.new(self.lstm.num_layers, batch_size, self.lstm.hidden_size).zero_(),\n","                  weight.new(self.lstm.num_layers, batch_size, self.lstm.hidden_size).zero_())\n","        return hidden\n","\n","# Hyperparameters\n","vocab_size = len(chars)\n","embedding_dim = 128\n","hidden_dim = 256\n","num_layers = 2\n","\n","model = LyricsLSTM(vocab_size, embedding_dim, hidden_dim, num_layers)\n","\n","optimizer = optim.Adam(model.parameters(), lr=0.001)\n","criterion = nn.CrossEntropyLoss()\n","print(model)\n"],"metadata":{"id":"y-EO9HTXikk3","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1720729249157,"user_tz":240,"elapsed":2029,"user":{"displayName":"Devon Peticolas","userId":"04023788229320726045"}},"outputId":"4e1677b5-127d-485f-ea7c-77505e889c6b"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["LyricsLSTM(\n","  (embedding): Embedding(82, 128)\n","  (lstm): LSTM(128, 256, num_layers=2, batch_first=True)\n","  (fc): Linear(in_features=256, out_features=82, bias=True)\n",")\n"]}]},{"cell_type":"markdown","source":["# Train our LSTM\n","\n","Start with epoch=1 and increase if you'd like more training\n","\n","```python\n","num_epochs = 1\n","\n","model.to(device)\n","model.train()\n","for epoch in range(num_epochs):\n","    hidden = model.init_hidden(64)\n","    for i, (X_batch, y_batch) in enumerate(dataloader):\n","        if len(X_batch) != 64: # This is a bug\n","            break\n","        X_batch = X_batch.to(device)\n","        y_batch = y_batch.to(device)\n","        optimizer.zero_grad()\n","        hidden = tuple([each.data for each in hidden])\n","        output, hidden = model(X_batch, hidden)\n","        loss = criterion(output, y_batch)\n","        loss.backward()\n","        optimizer.step()\n","        if i % 100 == 0:\n","            print(f'Epoch {epoch}/{num_epochs}, Batch {i}/{len(dataloader)}, Loss: {loss.item()}')\n","    print(f'Epoch {epoch + 1}/{num_epochs}, Loss: {loss.item()}')\n","```"],"metadata":{"id":"cLrqKHD-ila8"}},{"cell_type":"code","source":["num_epochs = 10\n","\n","model.to(device)\n","model.train()\n","for epoch in range(num_epochs):\n","    hidden = model.init_hidden(64)\n","    for i, (X_batch, y_batch) in enumerate(dataloader):\n","        if len(X_batch) != 64: # This is a bug\n","            break\n","        X_batch = X_batch.to(device)\n","        y_batch = y_batch.to(device)\n","        optimizer.zero_grad()\n","        hidden = tuple([each.data for each in hidden])\n","        output, hidden = model(X_batch, hidden)\n","        loss = criterion(output, y_batch)\n","        loss.backward()\n","        optimizer.step()\n","        if i % 100 == 0:\n","            print(f'Epoch {epoch}/{num_epochs}, Batch {i}/{len(dataloader)}, Loss: {loss.item()}')\n","    print(f'Epoch {epoch + 1}/{num_epochs}, Loss: {loss.item()}')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LgQeXtioTy4c","outputId":"ce592098-6294-4f10-e380-a6040563592c","executionInfo":{"status":"ok","timestamp":1720729387651,"user_tz":240,"elapsed":130212,"user":{"displayName":"Devon Peticolas","userId":"04023788229320726045"}}},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 0/10, Batch 0/1536, Loss: 4.416355609893799\n","Epoch 0/10, Batch 100/1536, Loss: 2.4865458011627197\n","Epoch 0/10, Batch 200/1536, Loss: 2.670980930328369\n","Epoch 0/10, Batch 300/1536, Loss: 2.2667417526245117\n","Epoch 0/10, Batch 400/1536, Loss: 2.381537437438965\n","Epoch 0/10, Batch 500/1536, Loss: 1.6872880458831787\n","Epoch 0/10, Batch 600/1536, Loss: 2.084592819213867\n","Epoch 0/10, Batch 700/1536, Loss: 1.7127478122711182\n","Epoch 0/10, Batch 800/1536, Loss: 1.8348757028579712\n","Epoch 0/10, Batch 900/1536, Loss: 2.165177345275879\n","Epoch 0/10, Batch 1000/1536, Loss: 1.9124559164047241\n","Epoch 0/10, Batch 1100/1536, Loss: 1.9726910591125488\n","Epoch 0/10, Batch 1200/1536, Loss: 1.4817055463790894\n","Epoch 0/10, Batch 1300/1536, Loss: 1.5168049335479736\n","Epoch 0/10, Batch 1400/1536, Loss: 1.9429641962051392\n","Epoch 0/10, Batch 1500/1536, Loss: 1.8944038152694702\n","Epoch 1/10, Loss: 1.5725018978118896\n","Epoch 1/10, Batch 0/1536, Loss: 1.5998836755752563\n","Epoch 1/10, Batch 100/1536, Loss: 1.6420053243637085\n","Epoch 1/10, Batch 200/1536, Loss: 1.271987795829773\n","Epoch 1/10, Batch 300/1536, Loss: 1.4738725423812866\n","Epoch 1/10, Batch 400/1536, Loss: 1.358452558517456\n","Epoch 1/10, Batch 500/1536, Loss: 1.4846680164337158\n","Epoch 1/10, Batch 600/1536, Loss: 1.9633418321609497\n","Epoch 1/10, Batch 700/1536, Loss: 1.2582792043685913\n","Epoch 1/10, Batch 800/1536, Loss: 1.7423548698425293\n","Epoch 1/10, Batch 900/1536, Loss: 1.6028523445129395\n","Epoch 1/10, Batch 1000/1536, Loss: 1.67611825466156\n","Epoch 1/10, Batch 1100/1536, Loss: 1.209140658378601\n","Epoch 1/10, Batch 1200/1536, Loss: 1.6084983348846436\n","Epoch 1/10, Batch 1300/1536, Loss: 1.294656753540039\n","Epoch 1/10, Batch 1400/1536, Loss: 1.4820692539215088\n","Epoch 1/10, Batch 1500/1536, Loss: 1.7120435237884521\n","Epoch 2/10, Loss: 1.6339740753173828\n","Epoch 2/10, Batch 0/1536, Loss: 1.609151005744934\n","Epoch 2/10, Batch 100/1536, Loss: 1.2573004961013794\n","Epoch 2/10, Batch 200/1536, Loss: 1.6758893728256226\n","Epoch 2/10, Batch 300/1536, Loss: 1.3938870429992676\n","Epoch 2/10, Batch 400/1536, Loss: 1.339969277381897\n","Epoch 2/10, Batch 500/1536, Loss: 1.1165211200714111\n","Epoch 2/10, Batch 600/1536, Loss: 1.4222006797790527\n","Epoch 2/10, Batch 700/1536, Loss: 1.466476321220398\n","Epoch 2/10, Batch 800/1536, Loss: 1.3714617490768433\n","Epoch 2/10, Batch 900/1536, Loss: 1.3326479196548462\n","Epoch 2/10, Batch 1000/1536, Loss: 1.551971673965454\n","Epoch 2/10, Batch 1100/1536, Loss: 1.106916904449463\n","Epoch 2/10, Batch 1200/1536, Loss: 1.3512015342712402\n","Epoch 2/10, Batch 1300/1536, Loss: 1.622028112411499\n","Epoch 2/10, Batch 1400/1536, Loss: 1.3490214347839355\n","Epoch 2/10, Batch 1500/1536, Loss: 1.0517264604568481\n","Epoch 3/10, Loss: 1.2151782512664795\n","Epoch 3/10, Batch 0/1536, Loss: 1.0208821296691895\n","Epoch 3/10, Batch 100/1536, Loss: 1.2794166803359985\n","Epoch 3/10, Batch 200/1536, Loss: 1.0818880796432495\n","Epoch 3/10, Batch 300/1536, Loss: 1.2269631624221802\n","Epoch 3/10, Batch 400/1536, Loss: 0.9593693614006042\n","Epoch 3/10, Batch 500/1536, Loss: 1.255087971687317\n","Epoch 3/10, Batch 600/1536, Loss: 1.0596953630447388\n","Epoch 3/10, Batch 700/1536, Loss: 1.113511562347412\n","Epoch 3/10, Batch 800/1536, Loss: 0.8710291385650635\n","Epoch 3/10, Batch 900/1536, Loss: 1.0491931438446045\n","Epoch 3/10, Batch 1000/1536, Loss: 1.3378794193267822\n","Epoch 3/10, Batch 1100/1536, Loss: 1.2476133108139038\n","Epoch 3/10, Batch 1200/1536, Loss: 1.5644824504852295\n","Epoch 3/10, Batch 1300/1536, Loss: 1.1424733400344849\n","Epoch 3/10, Batch 1400/1536, Loss: 1.1779602766036987\n","Epoch 3/10, Batch 1500/1536, Loss: 1.3467612266540527\n","Epoch 4/10, Loss: 1.3371801376342773\n","Epoch 4/10, Batch 0/1536, Loss: 1.0502252578735352\n","Epoch 4/10, Batch 100/1536, Loss: 1.1516540050506592\n","Epoch 4/10, Batch 200/1536, Loss: 0.8222525119781494\n","Epoch 4/10, Batch 300/1536, Loss: 0.8675342798233032\n","Epoch 4/10, Batch 400/1536, Loss: 0.9519399404525757\n","Epoch 4/10, Batch 500/1536, Loss: 1.1368733644485474\n","Epoch 4/10, Batch 600/1536, Loss: 1.1806384325027466\n","Epoch 4/10, Batch 700/1536, Loss: 0.987642765045166\n","Epoch 4/10, Batch 800/1536, Loss: 0.9070990681648254\n","Epoch 4/10, Batch 900/1536, Loss: 0.9215518832206726\n","Epoch 4/10, Batch 1000/1536, Loss: 0.9908875226974487\n","Epoch 4/10, Batch 1100/1536, Loss: 1.0652811527252197\n","Epoch 4/10, Batch 1200/1536, Loss: 0.9326328039169312\n","Epoch 4/10, Batch 1300/1536, Loss: 1.026176929473877\n","Epoch 4/10, Batch 1400/1536, Loss: 1.0095722675323486\n","Epoch 4/10, Batch 1500/1536, Loss: 1.1742844581604004\n","Epoch 5/10, Loss: 1.1750799417495728\n","Epoch 5/10, Batch 0/1536, Loss: 1.0192919969558716\n","Epoch 5/10, Batch 100/1536, Loss: 0.8477395176887512\n","Epoch 5/10, Batch 200/1536, Loss: 0.49430039525032043\n","Epoch 5/10, Batch 300/1536, Loss: 0.9721172451972961\n","Epoch 5/10, Batch 400/1536, Loss: 0.8168066740036011\n","Epoch 5/10, Batch 500/1536, Loss: 0.9285140037536621\n","Epoch 5/10, Batch 600/1536, Loss: 1.133998155593872\n","Epoch 5/10, Batch 700/1536, Loss: 1.1447464227676392\n","Epoch 5/10, Batch 800/1536, Loss: 0.9699601531028748\n","Epoch 5/10, Batch 900/1536, Loss: 0.851307213306427\n","Epoch 5/10, Batch 1000/1536, Loss: 0.7661038041114807\n","Epoch 5/10, Batch 1100/1536, Loss: 1.05207097530365\n","Epoch 5/10, Batch 1200/1536, Loss: 1.0346693992614746\n","Epoch 5/10, Batch 1300/1536, Loss: 0.9629153609275818\n","Epoch 5/10, Batch 1400/1536, Loss: 0.8365466594696045\n","Epoch 5/10, Batch 1500/1536, Loss: 0.7732195258140564\n","Epoch 6/10, Loss: 0.9590075016021729\n","Epoch 6/10, Batch 0/1536, Loss: 1.0674397945404053\n","Epoch 6/10, Batch 100/1536, Loss: 0.8144547939300537\n","Epoch 6/10, Batch 200/1536, Loss: 0.9042263031005859\n","Epoch 6/10, Batch 300/1536, Loss: 0.9020187854766846\n","Epoch 6/10, Batch 400/1536, Loss: 0.7865007519721985\n","Epoch 6/10, Batch 500/1536, Loss: 0.8367621302604675\n","Epoch 6/10, Batch 600/1536, Loss: 1.0470949411392212\n","Epoch 6/10, Batch 700/1536, Loss: 0.9065170288085938\n","Epoch 6/10, Batch 800/1536, Loss: 0.8634771108627319\n","Epoch 6/10, Batch 900/1536, Loss: 0.8321459889411926\n","Epoch 6/10, Batch 1000/1536, Loss: 0.8942782282829285\n","Epoch 6/10, Batch 1100/1536, Loss: 0.9911512136459351\n","Epoch 6/10, Batch 1200/1536, Loss: 0.9077779650688171\n","Epoch 6/10, Batch 1300/1536, Loss: 0.577753484249115\n","Epoch 6/10, Batch 1400/1536, Loss: 0.7472645044326782\n","Epoch 6/10, Batch 1500/1536, Loss: 0.8465446829795837\n","Epoch 7/10, Loss: 0.7146957516670227\n","Epoch 7/10, Batch 0/1536, Loss: 0.7987909913063049\n","Epoch 7/10, Batch 100/1536, Loss: 0.5506657361984253\n","Epoch 7/10, Batch 200/1536, Loss: 0.6387930512428284\n","Epoch 7/10, Batch 300/1536, Loss: 0.7586384415626526\n","Epoch 7/10, Batch 400/1536, Loss: 0.8428020477294922\n","Epoch 7/10, Batch 500/1536, Loss: 0.7848615646362305\n","Epoch 7/10, Batch 600/1536, Loss: 0.7644981741905212\n","Epoch 7/10, Batch 700/1536, Loss: 0.7800765633583069\n","Epoch 7/10, Batch 800/1536, Loss: 1.0267071723937988\n","Epoch 7/10, Batch 900/1536, Loss: 0.7621190547943115\n","Epoch 7/10, Batch 1000/1536, Loss: 0.7127704620361328\n","Epoch 7/10, Batch 1100/1536, Loss: 0.7023205161094666\n","Epoch 7/10, Batch 1200/1536, Loss: 0.9297565817832947\n","Epoch 7/10, Batch 1300/1536, Loss: 0.6092143058776855\n","Epoch 7/10, Batch 1400/1536, Loss: 0.7165383696556091\n","Epoch 7/10, Batch 1500/1536, Loss: 1.0925467014312744\n","Epoch 8/10, Loss: 0.832564651966095\n","Epoch 8/10, Batch 0/1536, Loss: 0.7420969009399414\n","Epoch 8/10, Batch 100/1536, Loss: 0.8098326325416565\n","Epoch 8/10, Batch 200/1536, Loss: 0.7861149907112122\n","Epoch 8/10, Batch 300/1536, Loss: 0.5643162727355957\n","Epoch 8/10, Batch 400/1536, Loss: 0.7357177138328552\n","Epoch 8/10, Batch 500/1536, Loss: 0.5801519751548767\n","Epoch 8/10, Batch 600/1536, Loss: 0.5516812205314636\n","Epoch 8/10, Batch 700/1536, Loss: 0.7403590083122253\n","Epoch 8/10, Batch 800/1536, Loss: 0.7787936925888062\n","Epoch 8/10, Batch 900/1536, Loss: 0.5724527835845947\n","Epoch 8/10, Batch 1000/1536, Loss: 0.608564019203186\n","Epoch 8/10, Batch 1100/1536, Loss: 0.7570845484733582\n","Epoch 8/10, Batch 1200/1536, Loss: 0.611545979976654\n","Epoch 8/10, Batch 1300/1536, Loss: 0.9566792845726013\n","Epoch 8/10, Batch 1400/1536, Loss: 0.47913822531700134\n","Epoch 8/10, Batch 1500/1536, Loss: 0.6707873940467834\n","Epoch 9/10, Loss: 0.7765668630599976\n","Epoch 9/10, Batch 0/1536, Loss: 0.40815305709838867\n","Epoch 9/10, Batch 100/1536, Loss: 0.673530638217926\n","Epoch 9/10, Batch 200/1536, Loss: 0.408420592546463\n","Epoch 9/10, Batch 300/1536, Loss: 0.6344121098518372\n","Epoch 9/10, Batch 400/1536, Loss: 0.5163803100585938\n","Epoch 9/10, Batch 500/1536, Loss: 0.5459937453269958\n","Epoch 9/10, Batch 600/1536, Loss: 0.6360874772071838\n","Epoch 9/10, Batch 700/1536, Loss: 0.44141390919685364\n","Epoch 9/10, Batch 800/1536, Loss: 0.5165444016456604\n","Epoch 9/10, Batch 900/1536, Loss: 0.5678737759590149\n","Epoch 9/10, Batch 1000/1536, Loss: 0.5085033178329468\n","Epoch 9/10, Batch 1100/1536, Loss: 0.7131974101066589\n","Epoch 9/10, Batch 1200/1536, Loss: 0.5059001445770264\n","Epoch 9/10, Batch 1300/1536, Loss: 0.5892197489738464\n","Epoch 9/10, Batch 1400/1536, Loss: 0.9545994400978088\n","Epoch 9/10, Batch 1500/1536, Loss: 0.5421246290206909\n","Epoch 10/10, Loss: 0.5086466073989868\n"]}]},{"cell_type":"markdown","source":["# Choose Your Starting Text and Generate Lyrics\n","\n","```python\n","model.eval()\n","\n","start_text = \"Once upon a time\"\n","length = 100\n","\n","chars = [char for char in start_text]\n","hidden = model.init_hidden(1)\n","for char in start_text:\n","    char_idx = torch.tensor([[char_to_idx[char]]], dtype=torch.long).to(device)\n","    output, hidden = model(char_idx, hidden)\n","char_idx = char_idx.squeeze(0)\n","\n","for _ in range(length):\n","    output, hidden = model(char_idx.unsqueeze(0), hidden)\n","    prob = torch.softmax(output, dim=1).data\n","    char_idx = torch.multinomial(prob, 1)[0]\n","    chars.append(idx_to_char[char_idx.item()])\n","\n","generated_text = ''.join(chars)\n","\n","print(generated_text)\n","```"],"metadata":{"id":"vQ_IBKaDis1m"}},{"cell_type":"code","source":["import time\n","model.eval()\n","\n","start_text = \"We're learning about LLMs and generation \"\n","length = 1000\n","\n","chars = [char for char in start_text]\n","hidden = model.init_hidden(1)\n","for char in start_text:\n","    char_idx = torch.tensor([[char_to_idx[char]]], dtype=torch.long).to(device)\n","    output, hidden = model(char_idx, hidden)\n","char_idx = char_idx.squeeze(0)\n","\n","for _ in range(length):\n","    output, hidden = model(char_idx.unsqueeze(0), hidden)\n","    prob = torch.softmax(output, dim=1).data\n","    char_idx = torch.multinomial(prob, 1)[0]\n","    print(idx_to_char[char_idx.item()], end=\"\")\n","    time.sleep(0.1)\n","\n","# generated_text = ''.join(chars)\n","\n","# print(generated_text)\n","\n","\n","\n","\n","\n"],"metadata":{"id":"szCB9Brci4gG","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1720729573224,"user_tz":240,"elapsed":101930,"user":{"displayName":"Devon Peticolas","userId":"04023788229320726045"}},"outputId":"244b1278-18e5-4d09-bc0b-224a32a998de"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["lose, at to be lome\n","And how this time you other\n","Like I was a \"mar)\n","With your heart room, baby\n","We could ever leave me, Mr. \"Perfect before you win still gots\n","With you I remember are train\n","I guess it couldn't be shit\n","Do the dark again of your body\n","It's for the man (Deain't down, did\n","I can get for me for body worked\n","Dawn red losin', precedes around you\"\n","\n","But I don't like a gate\n","I wanna be your A-Epeated 'tore red look at you around (You're being too my life, oh-oh\n","Baby, I I forget me to be smokens\n","At herp's crize, I do tell the only jeans, I\n","Can head now, go bry\n","And that's my man (Far), the goodbye it\n","But I enjoy would've been the purol, lover\n","I'm a vendending and crime, I was there\n","Are routes times, change in the kestic for mine\n","I'm still on that red, undidies is my baby\n","I'll be usin' for the better in the clandelf\n","Dow you like a game\n","\n","I can feel the other blaoks in a naster, I wole are back every time\n","'Cause we never go out of so grace\n","\n","And I've got a hunting red, hund, ah\n","Awer the were"]}]},{"cell_type":"code","source":[],"metadata":{"id":"s2KfG652URnc"},"execution_count":null,"outputs":[]}]}